{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the list of pdf file names and urls from the webpage\n",
      "\n",
      "import requests\n",
      "import re\n",
      "\n",
      "def get_pdf_files(env):\n",
      "\n",
      "    page = requests.get(env[\"budget_url\"]).content\n",
      "    \n",
      "    links = re.findall(r\"<a.*?\\s*href=\\\"(.*?)\\\".*?>(.*?)</a>\", page) \n",
      "    \n",
      "    #got all the links on the page, but need only pdf's\n",
      "    pdf_files = []\n",
      "   \n",
      "    for link in links: \n",
      "        if link[0].endswith(\".pdf\") or link[0].endswith(\"PDF\"):\n",
      "            pdf_file = link[0]\n",
      "            #remove spaces and braces from the file names\n",
      "            local_file = link[1].replace(\" \",\"\")\n",
      "            local_file = local_file.replace(\"(\",\"\")\n",
      "            local_file = local_file.replace(\")\",\"\")\n",
      "            local_file = local_file.replace(\"<br/>\",\"\")+\".pdf\"\n",
      "            # TODO: convert pdf_files to dictionary....\n",
      "            pdf_files.append( [\n",
      "                env[\"root_url\"]+pdf_file, \n",
      "                pdf_file, \n",
      "                env[\"dir_download\"]+local_file,\n",
      "                env[\"dir_ocr\"]+local_file,\n",
      "                env[\"dir_csv\"]+local_file,\n",
      "                env[\"dir_final\"]+local_file\n",
      "            ] )\n",
      "    return pdf_files        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download the files into local directory ./download\n",
      "import urllib\n",
      "\n",
      "def download_file(url, output_file_name):\n",
      "    print \"loading \", url, \" to \", output_file_name\n",
      "    r = urllib.urlretrieve(url, output_file_name)\n",
      "    print r\n",
      "    print \"done...\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Files that are not searchable (image) are OCR'ed by ABBYY\n",
      "import envoy\n",
      "import os\n",
      "import process\n",
      "\n",
      "def convert_to_searchable_format(file_name, output_file_name):\n",
      "    if is_searchable(file_name):\n",
      "        print \"The file \",file_name, \" is already searchable.\"\n",
      "    elif os.path.isfile(output_file_name):\n",
      "        print \"The file \",file_name, \" was already converted to \", output_file_name, \".  Skipping...\"\n",
      "    else:\n",
      "        process.recognizeFile(file_name, output_file_name, \"English\", \"pdfSearchable\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test if file is image or text\n",
      "# TODO: Replace with PyPDF.extractText() that returns empty string if not searchable\n",
      "import envoy\n",
      "\n",
      "def is_searchable(file_name):\n",
      "    r = envoy.run('strings ' + file_name + ' | grep Font')\n",
      "    if \"Font\" in r.std_out:\n",
      "        return True\n",
      "    else:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Determine number of pages\n",
      "# TODO: it throws a warning first time, check out why\n",
      "from PyPDF2 import PdfFileReader\n",
      "\n",
      "def num_pages( file_name ):\n",
      "    pdf = PdfFileReader(open( file_name ))\n",
      "    return ( pdf.getNumPages() )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tabula will split one CSV file per each page of PDF file.  \n",
      "# Need to figure out the name of each CSV file.\n",
      "def calcualte_csv_output_file_name(file_name, page_number=1):\n",
      "    elems = file_name.split('/')\n",
      "    elems.reverse()\n",
      "    output_file_name = elems[0]\n",
      "    output_file_name = output_file_name.replace(\".pdf\",\"_\"+str(page_number)+\".csv\")\n",
      "    return output_file_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Using tabula-extractor to convert searchable PDF to CSV\n",
      "\n",
      "import envoy\n",
      "import os\n",
      "def convert_page_to_csv(file_name,page_number=1 ):\n",
      "\n",
      "    if not is_searchable( file_name ):\n",
      "            print \"Can't convert non-searchable pdf \", file_name ,\" to csv.\"\n",
      "    else:   \n",
      "        # Ensure we have output directory\n",
      "        dir_csv = \"./csv/\"\n",
      "        if not os.path.exists(dir_csv):\n",
      "            os.makedirs(dir_csv)\n",
      "        \n",
      "        output_file_name = dir_csv+calcualte_csv_output_file_name(file_name, page_number)\n",
      "        \n",
      "        cmd = \"./tabula-extractor/bin/tabula\"\n",
      "        cmd += \" -p \" + str(page_number) #page number option\n",
      "        cmd += \" -f CSV \" #output format CSV\n",
      "        cmd += \" -n \" #non-spreadsheet verion\n",
      "        cmd += \" -o \"+output_file_name #output file name file_name_[1-9].csv\n",
      "        cmd += \" \" + file_name #name of file to convert\n",
      "        print cmd\n",
      "        rr = envoy.run(cmd)\n",
      "        print \"done...\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Main loop to go throught all the files...\n",
      "\n",
      "# This script will crate new directories\n",
      "# ./download -- to download files from the url\n",
      "# ./ocr      -- for files converted from non-searchable to searchable format (ABBYY)\n",
      "# ./csv      -- for files converted from PDF to CSV format (Tabula), one CSV file per PDF page\n",
      "# ./final    -- for final CSV, where tables spanning mult pages are merged into single CSV\n",
      "env = { \n",
      "    \"root_url\"       : \"http://www.cityofjerseycity.com\",\n",
      "    \"budget_url\"     : \"http://www.cityofjerseycity.com/pub-info.aspx?id=2430\",\n",
      "    \"dir_download\"   : \"./download/\",\n",
      "    \"dir_ocr\"        : \"./ocr/\",\n",
      "    \"dir_csv\"        : \"./test2/\",\n",
      "    \"dir_final\"      : \"./final/\" }\n",
      "\n",
      "pdf_files = get_pdf_files(env)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print pdf_files[:1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test one file\n",
      "download_file(pdf_files[0][0], pdf_files[0][2])\n",
      "convert_to_searchable_format(pdf_files[0][2], pdf_files[0][3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# To test ABBYY, commented out downlaod & TABULA converstion.\n",
      "# Will clean up in the next version.\n",
      "for pdf_file in pdf_files:\n",
      "    #download_file(pdf_file[0], pdf_file[2])\n",
      "    convert_to_searchable_format(pdf_file[2], pdf_file[3])\n",
      "    #np = num_pages(pdf_file[3])\n",
      "    #for page_number in range(1,np+1):\n",
      "    #    convert_page_to_csv(pdf_file[3],page_number)\n",
      "    #convert_page_to_csv( file_name , file_path, 11, \"test2/\" )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# More ABBYY tests...\n",
      "\n",
      "#convert_to_searchable_format(\"./download/CY2012BudgetAmendmentIntroduced.pdf\", \"./ocr/CY2012BudgetAmendmentIntroduced.pdf\")\n",
      "#convert_to_searchable_format(\"./download/AnnualFinancialStatement2012.pdf\",\"./ocr/largeTest.pdf\") #102 pages\n",
      "#convert_to_searchable_format(\"./download/FY2010AnnualAudit.pdf\",\"./ocr/superLargeTest.pdf\") #236 pages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Under construction....\n",
      "# def status_check(pdf_files):\n",
      "    \n",
      "#    for pdf_file in pdf_files:\n",
      "    \n",
      "#        np = num_pages(pdf_file[2])\n",
      "#        is_s = is_searchable(pdf_file[2])\n",
      "        \n",
      "#        info = {\n",
      "#                \"number_pages\" : np,\n",
      "#                \"is_searchable\": is_s\n",
      "#                }\n",
      "        \n",
      "#        if os.path.isfile(pdf_file[2]) : download = True\n",
      "#        if os.path.isfile(pdf_file[3]) : ocr = True\n",
      "        #if os.path.isfile(pdf_file[4]) : csv = True #This one is complicated\n",
      "        #if os.path.isfile(pdf_file[5]) : final = True \n",
      "#        status = {\n",
      "#                  \"download\": download,\n",
      "#                  \"ocr\"     : ocr,\n",
      "#                  \"csv\"     : \"na\",\n",
      "#                  \"final\"   : \"na\",\n",
      "#                  }\n",
      "        \n",
      "#        pdf_files[\"pdf_file\"] += info\n",
      "#        pdf_files[\"pdf_file\"] += status\n",
      "#        */"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Detect tables running on muliple pages and merge the CSV files, so it's one per table per PDF file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data scraping, converting into structured data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download CSV and .json into https://data.openjerseycity.org/dataset/jersey-city-2013-budget-adopted-spending"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}